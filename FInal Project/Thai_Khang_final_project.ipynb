{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b22e391",
   "metadata": {},
   "source": [
    "<center><h1>Thai_Khang_Final_Project</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fa39f",
   "metadata": {},
   "source": [
    "Name: Khang Thai\n",
    "<br>\n",
    "Github Username: kunfupen\n",
    "<br>\n",
    "USC ID: 5721113147"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2feea3",
   "metadata": {},
   "source": [
    "## 1. Transfer Learning for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a25f8f",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e052b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, EfficientNetB0, VGG16, DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cdf41",
   "metadata": {},
   "source": [
    "### (a) In this problem, we are trying to build a classifier that distinguishes images of 17 types of jute pest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aed41",
   "metadata": {},
   "source": [
    "### (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf89e5",
   "metadata": {},
   "source": [
    "#### (i) One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258a120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6443 images belonging to 17 classes.\n",
      "Found 413 images belonging to 17 classes.\n",
      "Found 379 images belonging to 17 classes.\n",
      "Training samples: 6443\n",
      "Validation samples: 413\n",
      "Testing samples: 379\n"
     ]
    }
   ],
   "source": [
    "ZIP_FILEPATH = '..\\FInal Project\\Jute_Pest_Dataset.zip'\n",
    "EXTRACT_DIR = 'data'\n",
    "\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(ZIP_FILEPATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "\n",
    "TRAIN_DIR = os.path.join(EXTRACT_DIR, 'train')\n",
    "TEST_DIR = os.path.join(EXTRACT_DIR, 'test')\n",
    "VAL_DIR = os.path.join(EXTRACT_DIR, 'val')\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "temp_datagen = ImageDataGenerator()\n",
    "\n",
    "train_temp = temp_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_temp = temp_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_temp = temp_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", train_temp.n)\n",
    "print(\"Validation samples:\", val_temp.n)\n",
    "print(\"Testing samples:\", test_temp.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526186f",
   "metadata": {},
   "source": [
    "#### (ii) Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b4bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique image sizes found\n",
      "Resized 6444 training images.\n",
      "Resized 413 validation images.\n",
      "Resized 379 testing images.\n",
      "Found 6443 images belonging to 17 classes.\n",
      "Found 413 images belonging to 17 classes.\n",
      "Found 379 images belonging to 17 classes.\n",
      "Train Samples: 6443\n",
      "Validation Samples: 413\n",
      "Test Samples: 379\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "class_dir = os.listdir(TRAIN_DIR)\n",
    "\n",
    "for class_name in class_dir[:5]:\n",
    "    class_path = os.path.join(TRAIN_DIR, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)[:10]\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            with Image.open(img_path) as img:\n",
    "                sizes.append(img.size)\n",
    "\n",
    "unique_sizes = list(set(sizes))\n",
    "\n",
    "if len(unique_sizes) > 1:\n",
    "    widths = [size[0] for size in sizes]\n",
    "    heights = [size[1] for size in sizes]\n",
    "    print(\"Unique image sizes found\")\n",
    "\n",
    "\n",
    "RESIZED_DIR = Path('resized_data')\n",
    "TARGET_SIZE = (224, 224)\n",
    "\n",
    "def resize_images(src_dir, dst_dir, target_size):\n",
    "    src_path = Path(src_dir)\n",
    "    dst_path = Path(dst_dir)\n",
    "    image_files = list(src_path.rglob('*.*'))\n",
    "    for img_file in image_files:\n",
    "        relative_path = img_file.relative_to(src_path)\n",
    "        target_file_path = dst_path / relative_path\n",
    "        target_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        img = load_img(str(img_path), target_size=target_size)\n",
    "        img.save(str(target_file_path))   \n",
    "\n",
    "    return len(image_files)\n",
    "\n",
    "train_count = resize_images(TRAIN_DIR, RESIZED_DIR / 'train', TARGET_SIZE)\n",
    "val_count = resize_images(VAL_DIR, RESIZED_DIR / 'val', TARGET_SIZE)\n",
    "test_count = resize_images(TEST_DIR, RESIZED_DIR / 'test', TARGET_SIZE)\n",
    "\n",
    "print(f\"Resized {train_count} training images.\")\n",
    "print(f\"Resized {val_count} validation images.\")\n",
    "print(f\"Resized {test_count} testing images.\")\n",
    "\n",
    "TRAIN_DIR = str(RESIZED_DIR / 'train')\n",
    "VAL_DIR = str(RESIZED_DIR / 'val')\n",
    "TEST_DIR = str(RESIZED_DIR / 'test')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "print(\"Train Samples:\", train_generator.n)\n",
    "print(\"Validation Samples:\", val_generator.n)\n",
    "print(\"Test Samples:\", test_generator.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a8cf1",
   "metadata": {},
   "source": [
    "### (c) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb81df",
   "metadata": {},
   "source": [
    "#### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7356b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 - Total params: 24116625, Trainable params: 528913, Frozen params: 23587712\n",
      "ResNet101 - Total params: 43187089, Trainable params: 528913, Frozen params: 42658176\n",
      "EfficientNetB0 - Total params: 4381876, Trainable params: 332305, Frozen params: 4049571\n",
      "VGG16 - Total params: 14850385, Trainable params: 135697, Frozen params: 14714688\n",
      "DenseNet201 - Total params: 18818129, Trainable params: 496145, Frozen params: 18321984\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 17\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    "\n",
    "def create_model(base_model, num_classes=17):\n",
    "\n",
    "    if base_model == 'ResNet50':\n",
    "        base = ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'ResNet101':\n",
    "        base = ResNet101(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'EfficientNetB0':\n",
    "        base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'VGG16':\n",
    "        base = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'DenseNet201':\n",
    "        base = DenseNet201(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported base model\")\n",
    "    \n",
    "    base.trainable = False\n",
    "\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base.input, outputs=predictions)\n",
    "    return model, base\n",
    "\n",
    "models = {}\n",
    "base = {}\n",
    "model_names = ['ResNet50', 'ResNet101', 'EfficientNetB0', 'VGG16', 'DenseNet201']\n",
    "for name in model_names:\n",
    "    model, base_model = create_model(name, NUM_CLASSES)\n",
    "    models[name] = model\n",
    "    base[name] = base_model\n",
    "\n",
    "    tot_params = model.count_params()\n",
    "    trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    frozen_params = tot_params - trainable_params\n",
    "\n",
    "    print(f\"{name} - Total params: {tot_params}, Trainable params: {trainable_params}, Frozen params: {frozen_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b86a1",
   "metadata": {},
   "source": [
    "#### (ii) Empirical Reguarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8282a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6443 images belonging to 17 classes.\n",
      "Augmented Train Samples: 6443\n"
     ]
    }
   ],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Augmented Train Samples:\", train_generator_augmented.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e57b68",
   "metadata": {},
   "source": [
    "#### (iii) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664918c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6443 images belonging to 17 classes.\n",
      "Found 413 images belonging to 17 classes.\n",
      "Found 379 images belonging to 17 classes.\n",
      "Train Samples: 6443\n",
      "Validation Samples: 413\n",
      "Test Samples: 379\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "L2_LAMBDA = 0.01\n",
    "\n",
    "def create_model_iii(base_model, num_classes=17):\n",
    "\n",
    "    if base_model == 'ResNet50':\n",
    "        base = ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'ResNet101':\n",
    "        base = ResNet101(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'EfficientNetB0':\n",
    "        base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'VGG16':\n",
    "        base = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    elif base_model == 'DenseNet201':\n",
    "        base = DenseNet201(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
    "    \n",
    "    base.trainable = False\n",
    "\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(L2_LAMBDA))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(L2_LAMBDA))(x)\n",
    "\n",
    "    model = Model(inputs=base.input, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "models = {}\n",
    "model_names = ['ResNet50', 'ResNet101', 'EfficientNetB0', 'VGG16', 'DenseNet201']\n",
    "\n",
    "for name in model_names:\n",
    "    model = create_model_iii(name, NUM_CLASSES)\n",
    "    models[name] = model\n",
    "\n",
    "train_generator = train_datagen_augmented.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "print(\"Train Samples:\", train_generator.n)\n",
    "print(\"Validation Samples:\", val_generator.n)\n",
    "print(\"Test Samples:\", test_generator.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41c906",
   "metadata": {},
   "source": [
    "#### (iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba1532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.0673 - loss: 4.9870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 179ms/step - accuracy: 0.0706 - loss: 4.0408 - val_accuracy: 0.0073 - val_loss: 6.4248\n",
      "Epoch 2/50\n",
      "\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.0797 - loss: 3.3151"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 182ms/step - accuracy: 0.0823 - loss: 3.2722 - val_accuracy: 0.0145 - val_loss: 6.8199\n",
      "Epoch 3/50\n",
      "\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 192ms/step - accuracy: 0.0841 - loss: 3.1390 - val_accuracy: 0.0145 - val_loss: 5.3319\n",
      "Epoch 4/50\n",
      "\u001b[1m 398/1289\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 181ms/step - accuracy: 0.0894 - loss: 3.0706"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "\n",
    "hist = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    callback = [\n",
    "        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(f'best_model_{model_name}.keras', monitor='val_accuracy', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = models[model_name].fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callback,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    hist[model_name] = history\n",
    "\n",
    "    print(f\"{model_name} Training Time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    history = hist[model_name]\n",
    "    \n",
    "    axes[idx].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[idx].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[idx].set_title(f'{model_name}')\n",
    "    axes[idx].set_xlabel('Epochs')\n",
    "    axes[idx].set_ylabel('Loss')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid()\n",
    "\n",
    "if len(model_names) < 6:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.subtitle('Training and Validation Loss vs Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    history = hist[model_name]\n",
    "    \n",
    "    axes[idx].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[idx].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[idx].set_title(f'{model_name}')\n",
    "    axes[idx].set_xlabel('Epochs')\n",
    "    axes[idx].set_ylabel('Accuracy')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid()\n",
    "\n",
    "if len(model_names) < 6:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.subtitle('Training and Validation Accuracy vs Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
